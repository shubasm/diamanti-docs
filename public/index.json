[
{
	"uri": "http://www.diamanti.com/docs/cli_reference/",
	"title": "Command line interface guide",
	"tags": [],
	"description": "",
	"content": "Diamanti D-Series command line interface. This document provides a comprehensive reference to the Diamanti command line interface (CLI). The Diamanti CLI offers a set of commands that makes it easy to manage Diamanti D-Series clusters, nodes (appliances), storage volumes, networks, and more.\n"
},
{
	"uri": "http://www.diamanti.com/docs/getting-started/",
	"title": "Diamanti D-Series quick start guide",
	"tags": [],
	"description": "",
	"content": "The Diamanti D-Series Quick start guide introduces the Diamanti D-Series appliance and provides an overview of the principal components and features of the Diamanti software. This guide then describes how to perform the most common operations, including how to create a cluster, define Diamanti objects such as networks and storage volumes, and deploy workloads using Kubernetes.\n"
},
{
	"uri": "http://www.diamanti.com/docs/getting-started/quickstart/",
	"title": "Quick start guide",
	"tags": [],
	"description": "",
	"content": "Audience\nThis document is designed for the following users:\n  Cluster administrators\n  User administrators\n  Storage administrators\n  Network administrators\n  Developers\n  Cluster, storage, and network administrators ensure the proper deployment and daily operation of an organization’s IT infrastructure, including Diamanti D-Series appliances. Developers, meanwhile, deploy containers on Diamanti appliances.\nIntroducing the Diamanti D-Series Appliance Diamanti provides a hyperconverged platform for running Linux containers that integrates with open source software including Docker and Kubernetes. The D-Series appliance, along with Diamanti software, offers developers and administrators on-demand access to fast, predictable data services. This allows developers to specify I/O policies on a per-container or per-application basis, as needed.\nThese policies link standard applications such as databases (for example, MongoDB, Cassandra, MySQL, and Kafka), networking services (such as NGINX), and CI/CD applications (for instance, Jenkins) with software-defined network and storage resources, ensuring that the I/O requirements for each application are fulfilled.\nThis provides administrators with an easy-to-use, standards-based container platform.\nUnderstanding D-Series Appliance Basics The Diamanti D-Series appliance is purpose-built for containerized applications that combines a hyperconverged infrastructure with the performance and efficiency of bare-metal containers. The Diamanti software provides a virtualized, clustered approach to network and storage.\nTogether, this provides containerized workloads with guaranteed network and storage performance to complement the CPU and memory allocation provided by Linux containers. These guarantees allow more workloads to run on the same class of hardware without sacrificing application quality, reliability, or performance.\nDiamanti D-Series appliances assure network performance by enforcing performance tiers at the network interface for each container. Each container is guaranteed a share of available network bandwidth. Similarly, D-Series appliances ensure storage performance through bandwidth, IOPS, and latency guarantees. These guarantees are enforced in hardware, solving issues related to noisy neighbors in multitenancy environments.\nIntroducing Diamanti D-Series Clusters The Diamanti software aggregates resources from individual member nodes and groups the nodes into a cluster. After a Diamanti cluster is formed, a virtual IP address (VIP) is assigned to the cluster, which provides access to a single point of management.\nThis virtual IP address is also referred to as the management IP address for the cluster in the\nDiamanti documentation.\nAdministrators have access to both a command line interface and an easy-to-use browser-based user interface to manage and monitor the cluster.\nNote: The Diamanti D-Series cluster is built with a typical quorum size of three nodes.\nExploring Container Networking The Diamanti software provides centralized management of network configurations, which are then realized across a distributed cluster. Diamanti addresses application networking needs to meet both the connectivity and service requirements of the management network and the performance requirements of applications using data networks.\nThe Diamanti software does this by providing capabilities to attach multiple interfaces to a single container. This approach further allows network administrators to maintain isolation between management and data networks.\nContainer Management Network By default, the Diamanti software provides a management network for containers allowing service discovery. This management network is a static subnet (172.20.0.0/24) on each node.\nContainer Data Networks Each container can request one or more interfaces on specified data networks. Further, each data interface can be programmed to have a specific performance configuration. The Diamanti software provisions the data network requirements using SR-IOV NICs on the nodes.\nFor data networking, the Diamanti software employs two related objects: networks and endpoints.\nNetworks Diamanti network objects comprise IP address pools from which containers can have IP addresses allocated. The Diamanti software provides layer 2 (L2) networking with VLANs.\nDiamanti network objects allow network administrators to configure the following parameters:\n  Network name, which is a user-defined string that identifies the network object\n  Subnet for the IP address range\n  IP address range\n  Default gateway for the subnet (optional)\n  VLAN corresponding to the subnet (which is programmed on the Top-of-Rack switch)\n  These parameters allow network administrators to address most network topologies, including the following:\n  Public networks—Required if containers need to communicate with external components on a different subnet. In this case, you can configure the gateway, as needed.\n  Private networks—Available if containers do not need to communicate with external com- ponents, or are running in a restricted environment. In this case, network administrators should not configure the gateway.\n  Note that private networks allow users to reuse the same subnet in multiple clusters without conflict as long as the ToR switches are appropriately configured for the VLAN.\nEndpoints Container networking is provisioned through endpoints. Endpoints associate container networking requirements with network objects and performance guarantees. Endpoints are allocated their IP address from the specified networks. Each endpoint uses a dedicated SR-IOV virtual function (VNIC) to provide networking for the container.\nEndpoints can be either persistent (named) or dynamic. Users need to explicitly create or delete persistent endpoints. Because of this, persistent endpoints are suitable for services that require a fixed IP address. Dynamic endpoints, in contrast, are created when a pod is created, and deleted automatically when the pod is deleted.\nNote: MAC addresses are automatically assigned without the need for user configuration.\nExploring Storage The Diamanti software includes an advanced distributed volume manager that pools storage from drives available on all appliances in the cluster.\nEach volume uses dedicated SR-IOV virtual function to provide storage for the container.\nDiamanti Converged File System Diamanti Converged File System (DCFS) is the distributed storage component of the Diamanti cluster. DCFS consists of a highly-available and distributed control plane that scales linearly when the number of nodes in the cluster increases.\nDCFS provides application-centric storage features that include mirroring, remote I/O, snapshots, and more. DCFS is network aware and automatically reserves network bandwidth as needed to maintain application quality of service. DCFS is flash media aware and designed for higher flash endurance and predictable I/O latency.\nVolumes Volumes of various sizes can be created from the distributed storage pool on demand. When a volume is created, the Diamanti scheduler automatically picks an optimum node on which to create the volume.\nRemote Volumes Volumes created on a node are accessible by other nodes using the Ethernet fabric. These remotely accessed volumes are referred to as remote volumes.\nNote that the Diamanti scheduler prefers local volumes and attempts to place containers on the nodes where the volumes are located.\nMirrored Volumes Volumes can be created to have mirrored copies on nodes (up to three copies). This ensures that there is no loss of data if a node or drive fails.\nSnapshots A snapshot represents a point-in-time image of the corresponding volume data. Diamanti snapshots are read-only. Users need to create a volume from a snapshot to consume snapshot data.\nLinked Clones Volumes created from Diamanti snapshots are called linked clones of the parent volume. Linked clones share data blocks with the corresponding snapshot until the linked clone blocks are modified. Linked clones can be attached to any node in the cluster for consumption.\nDynamic Provisioning The Diamanti software offers a dynamic provisioner for Kubernetes, allowing storage administrators to dynamically provision storage volumes. Dynamic provisioning uses the concept of storage classes, which enables storage administrators to define classes of storage for use with varying application requirements.\nUnderstanding Namespaces The Diamanti software supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called namespaces. Namespaces are helpful in environments with many users spread across multiple teams or projects. In this case, namespaces provide a private workspace for each team or deployment.\nNamespaces do this by enforcing a scope for names. When using namespaces, names of resources need to be unique within a namespace, but not across namespaces. In this way, namespaces allow cluster administrators to divide cluster resources between multiple uses.\nUnderstanding Performance Tiers The Diamanti software allows users to run applications using different performance tiers based on specific service-level agreement (SLA) requirements. By default, the Diamanti software offers three predefined performance tiers. Cluster administrators can add new performance tiers, up to a total of eight, based on user requirements.\nUser Access Management The Diamanti software provides built-in role-based access control (RBAC) to regulate access to resources within the environment, providing a streamlined and secure mechanism to perform cluster, container, and user administration. The Diamanti software offers several built-in roles and\ngroups to cover the most common administrative tasks. Cluster administrators can also create custom groups to match specific needs, as well as define users within the cluster.\nThe Diamanti software also supports the namespaces construct in Kubernetes to manage access to application and service-related Kubernetes objects. These objects include pods, services, replication controllers, deployments, and other objects that are created in namespaces.\nDiamanti roles are expressed using the following format:\n\u0026lt;class-\u0026lt;qualifier[/\u0026lt;namespace]\nwhere \u0026lt;class\\ refers to the object type (such as container, network, or node, among others),\n\u0026lt;qualifier\\ specifies the permitted operations (view or edit), and \u0026lt;namespace\\ identifies the namespace to which the role applies.\nFor example, built-in roles include network-edit and container-edit/project, among others.\nCluster administrators can configure additional RBAC privileges for Kubernetes objects using Kubernetes role and role bindings. Diamanti built-in roles and bindings are identified using the label diamanti.com/created-by. Custom roles and bindings must use a different label.\nKubernetes The D-Series appliance is packaged with a certified version of Kubernetes (version 1.12.3). The Diamanti network CNI plug-in and the storage plug-in (via Flex-volume/CSI) are pre-integrated and tested with the Kubernetes distribution.\nNote: Do not upgrade the Kubernetes software independently. The Kubernetes software is upgraded, if necessary, as part of the Diamanti software upgrade process.\nFor more information on Kubernetes and supported features, refer to the Kubernetes documentation.\nManaging the D-Series Appliance This chapter describes how to manage the Diamanti D-Series appliance. The chapter begins with an overview of how to deploy the Diamanti D-Series appliance, and then provides detailed instructions about how to cluster multiple D-Series appliances.\nThe chapter further explains how to monitor and manage the cluster, as well as how to manage groups of users. The chapter then provides information about managing key Diamanti objects including performance tiers, networks, endpoints, and storage. The chapter also describes how to monitor the health of the D-Series appliance.\nThe chapter concludes by showing how to deploy workloads as containers using Kubernetes pod definitions, and how to use Kubernetes commands to run, monitor and delete pods.\nDeploying the D-Series Appliance Deploying the Diamanti D-Series appliance is a quick and simple process that involves the following steps:\n  Becoming familiar with the Diamanti D-Series package contents and collecting the neces- sary hardware and configuration information\n  Physically installing the D-Series appliance in the data center\n  Configuring the D-Series appliance to prepare it to join a cluster\n  For more information about deploying a Diamanti D-Series appliance, refer to the Diamanti D- Series Installation Guide.\nClustering D-Series Appliances Cluster administrators can create a cluster of D-Series appliances using the Diamanti software, and add or remove nodes in the cluster as needed. After a cluster is formed, the Diamanti software pools resources across all nodes in the cluster, enabling Kubernetes to efficiently schedule containers within the cluster.\nNote: Complete the Diamanti Cluster Preparation Checklist, available in the Diamanti D-Series Installation Guide, before configuring a cluster.\nCluster administrators interact with the Diamanti software using the Diamanti command line interface (CLI), which provides a set of commands to manage Diamanti D-Series clusters, nodes (appliances), storage volumes, networks, and more. In contrast, users employ standard Kubernetes commands to manage pods, containers, and other Kubernetes resources in the cluster.\nUse the Diamanti command line tool dctl (Linux and Mac OS X) and dctl.exe (Windows) to issue Diamanti commands, and use the Kubernetes command line tool kubectl (Linux and Mac OS X) and kubectl.exe (Windows) to issue Kubernetes commands.\nDiamanti refers to these tools as dctl and kubectl respectively throughout the documentation.\nDiamanti strongly recommends running these tools on a local Linux, Windows, or Mac OS X workstation. Users can download the Diamanti tools to run the Diamanti CLI on a local machine. For more information about downloading the tools, refer to the Diamanti D-Series User Interface Guide.\nPreparing to Cluster Appliances Before creating a cluster, ensure that the host names and cluster name have DNS entries for the corresponding domain. If these DNS entries are not present (and therefore cannot be resolved by nodes), attempts to create the cluster will fail.\nTo verify that the host names and cluster name have DNS entries, do the following:\n  Using a console monitor, log in to the node.\n  Confirm that nameserver entries are correctly configured using the following command:\n  $ cat /etc/resolv.conf\nThe resolv.conf file typically contains directives that specify the default search domains along with the list of IP addresses of nameservers available for resolution.\n Perform a DNS query to test the address mapping of a well-known site. For example:  \\$ nslookup google.com\nCreating a Cluster To create a secure cluster, cluster administrators need to specify the virtual IP address for the cluster, the DNS sub-domain, the VLAN used for intra-cluster storage communication, an admin user password for the cluster, the certificate authority for the cluster, and the TLS certificate and private key for the cluster.\nAll commands, except the dctl cluster create command, require administrators to be logged into the cluster (using the dctl login command).\nUse the following command:\nThe following describes the command parameters:\n   Parameter Description     --admin-password \u0026lt;password\\ -p \u0026lt;password\\ The password for the admin user. If a password is not specified in the command line, the command prompts for a password.   -c \u0026lt;path-to-config-file\\ The path to the cluster creation configuration file con- taining the cluster name, admin user password, certifi- cate authority, server private key, storage VLAN, virtual IP address, and list of nodes.   --ca-cert value \u0026lt;path-to-certificate\\ The certificate authority for the cluster.   \u0026lt;cluster-name\\ The name of the cluster.   \u0026lt;dns-name\\ The DNS (short) name of the node.   --poddns \u0026lt;cluster-name[.company .domain] The domain name for the DNS service for all contain- ers deployed in the cluster. The default is domain.com.   -s \u0026lt;node-name | node-ip-address\\ The DNS (short) name or IP address of the Diamanti server that is a member of the cluster. Use when run- ning the command from a Linux, Windows, or Mac- based machine.   --storage-vlan [vlan-id] \u0026ndash;svlan [vlan-id] The virtual LAN ID (for intra-cluster storage communi- cation, with jumbo frames enabled). The default value is 0 (disabled).   --tls-cert value \u0026lt;path-to-certificate\\ The TLS certificate for the cluster.   --tls-key value \u0026lt;path-to-key-file\\ The TLS private key for the cluster.   --vip \u0026lt;cluster-virtual-ip\\ \u0026ndash;virtual-ip \u0026lt;cluster-virtual-ip\\ The IP address for managing the cluster.    The password needs to meet the Unicode standard, and must consist of at least 8 characters, including at least one special character, one upper and lowercase character, and one number.\nSee Appendix A for an example of a configuration file.\nThe command defaults to 127.0.0.1 if the node IP address is not specified. Therefore, the node IP address is not required when the command is run on a node that is part of the cluster.\nFor example:\n NAME NODE-STATUS K8S-STATUS MILLICORES MEMORY STORAGE IOPS VNICS BANDWIDTH SCTRLS LABELS LOCAL, REMOTE appserv76 0/0 0/0 0/0 0/0 0/0 0/0 0/0, 0/0 \u0026lt;none\u0026gt; appserv77 0/0 0/0 0/0 0/0 0/0 0/0 0/0, 0/0 \u0026lt;none\u0026gt; appserv78 0/0 0/0 0/0 0/0 0/0 0/0 0/0, 0/0 \u0026lt;none\u0026gt; Note: Do not add spaces or other whitespace characters when specifying the list of nodes (using DNS short names).\nThe dctl cluster create command automatically adds an administrative user named ‘admin’.\nUsing the default quorum size of three nodes, the first three nodes specified in the\ndctl cluster create command become quorum nodes by default.\nLogging in to the Cluster Users need to be logged in to the cluster before performing CLI operations. There are two ways to log in to a cluster:\n  Insecure mode—Use when user-supplied certificates were used to create the cluster and the certificate authority (ca.crt) is not available for the user. This allows the user to config- ure dctl to ignore server certificate validation. This is the default login mode.\n  Kubernetes proxy mode—In certain environments (such as with an SSL passthrough load- balancer), kubectl commands are not properly relayed to the API server causing kubectl commands to fail with an error message. In these cases, use the proxy option, which relays kubectl commands through the Diamanti server.\n  Use the following command to log in to the Diamanti cluster:\ndctl [-s \\\u0026lt;cluster-vip \\| cluster-dns-name\\] login\nUsers are prompted for the user name and the password. For example:\nAfter successfully logging in to the cluster, the dctl login command creates a D-Series cluster session cookie in the .dctl.d directory. Note that this cookie is deleted when users log out using dctl logout command.\nThe dctl login command also creates a .kube/config file that contains the Kubernetes cookie together with the context of the D-Series cluster. Users can interact with the Kubernetes API Server after this information is available.\nUsers can determine the identity of the user currently logged in using the following command:\n dctl whoami\nFor example:\nTo log out of the cluster, use the following command:\n\\$ dctl logout\nCLI Configuration Management After successfully logging in to the cluster, the dctl login command creates D-Series cluster session cookies, which are deleted when users log out using the dctl logout command. The dctl login command also creates Kubernetes session tokens that contain the Kubernetes cookie together with the context of the D-Series cluster.\nThese session tokens are stored under the home directory in the following files:\n   File Description     ~/.dctl.d/cookies Diamanti session cookies   ~/.kube/config Kubernetes session tokens    Note: The user session tokens expire in one hour.\nUsers can specify alternative locations for these configuration files using the following commands:\nFor example:\nNote: If you export DCTL_CONFIG, you need to also export KUBECONFIG as shown above.\nWhen destroying the cluster, users should remove the corresponding folders on the machine before using the Diamanti CLI (dctl commands) again.\nMonitoring and Managing the Cluster After creating the cluster and logging in to it, users can monitor the cluster status, add more nodes to the cluster, label nodes, and more.\nUse the following command to display the status of the cluster:\ndctl cluster status\nFor example:\n \\$ dctl cluster status Name : production-cluster UUID : efbe3403-661d-11e9-aff6-2c600c82ec99 State : Created Version : 2.3.0 (108) Master : appserv3 Etcd State : Healthy Virtual IP : 172.16.19.21 Storage VLAN : 402 Pod DNS Domain : cluster.local NAME NODE-STATUS K8S-STATUS MILLICORES MEMORY STORAGE IOPS VNICS BANDWIDTH SCTRLS LABELS LOCAL, REMOTE appserv2/(etcd) Failed Good 0/32000 1GiB/64GiB 200.54GB/3.05TB 185K/500K 20/63 4.63G/40G 20/64, 0/64 beta.kubernetes. io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=appserv2 appserv3/(master, etcd) Good Good 250/32000 1.06GiB/ 64GiB 258.36GB/3.05TB 185K/500K 25/63 4.63G/40G 21/64, 0/64 beta.kubernetes. io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=appserv3 appserv4/(etcd) Good Good 0/32000 1GiB/64GiB 1.07TB/3.05TB 185K/500K 24/63 4.63G/40G 20/64, 0/64 beta.kubernetes. io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=appserv4 Use the following command to add one or more nodes to a cluster:\ndctl cluster add \\\u0026lt;dns-name\\,[\\\u0026lt;dns-name\\,...]\nFor example:\n\\$ dctl cluster add host1,host2\nImportant: Do not add spaces or other whitespace characters when specifying the list of nodes (using DNS short names).\nSimilarly, use the following command to remove one or more nodes from a cluster:\ndctl cluster remove \u0026lt;dns-name,[\u0026lt;dns-name,\u0026hellip;]\nFor example:\n\\$ dctl cluster remove host1\nYou are prompted for confirmation.\nUse the following command to add a label to a node:\ndctl node label \\\u0026lt;node-name\\ \\\u0026lt;label\\\nFor example:\nManaging User Groups The Diamanti software uses role-based access control (RBAC) to regulate access to resources within the environment. As part of this system, Diamanti D-Series appliances employ the following related concepts:\n  Users—People who can perform tasks within the environment.\n  Roles—Collections of functions that users can perform. Within the Diamanti software, these functions are defined as methods that can act on objects within the system. There is a fixed set of predefined roles that cannot be changed.\n  Groups—Collections of users that assume the same role within the system. User groups define the privileges that users are assigned in the system.\n  Users can belong to multiple groups, and groups can have multiple roles.\nThe Diamanti software offers built-in roles and groups to cover the most common administrative tasks. Cluster administrators can also create custom groups to match specific needs, as well as define users within the cluster.\n   Object Name Description     User admin Belongs to the cluster-admin group.   Object Name Description   Group cluster-admin Has edit and view privileges for Diamanti resources including containers, volume claims, net- works, nodes, performance tiers, and volumes. Note that this group offers full privileges in Kubernetes.    container-admin Has edit privileges for contain- ers in the default namespace and view privileges for Diamanti resources including networks, nodes, performance tiers, and volumes.    user-admin Has view and edit privileges for users, group, and authentication servers.    Cluster administrators generally perform the following tasks:\n  Manage the cluster configuration\n  Monitor container deployments\n  Monitor hosts, volumes, and networks\n  Monitor and manage common operations of the IT infrastructure, including Diamanti D- Series appliances\n  Use the following command to display the list of roles associated with the user:\nUse the following command to create a new group with role based access control:\nUse the following command to get details about a particular group:\nUse the following command to create a new user and add the user to multiple user groups:\nUser names need to be in the following format: user@domain.\nPerforming Authentication Local users can be authenticated, as shown in the following example:\nUsing LDAP Authentication Users can be authenticated through an external LDAP server. The following shows an example of remote user authentication:\nManaging Performance Tiers Administrators can configure the Diamanti software to enforce minimum network throughput and storage IOPS for containers, offering deterministic high performance with high workload density. While the configuration is completed globally across the cluster, each node in the cluster enforces the specified performance tier for all workloads running on the node.\nThe following table outlines the three built-in performance tiers in a Diamanti cluster:\n   Performance Tier Storage IOPS Network Bandwidth     high 20K IOPS 500 Mbps   medium 5K IOPS 125 Mbps   best-effort No minimum No minimum    Administrators can create new performance tiers to match application requirements using the Diamanti CLI. Administrators can define five additional performance tiers (for a total of eight tiers). Administrators cannot modify or delete the default built-in performance tier “best-effort.” If a performance tier is not specified, the default is “best-effort.”\nDiamanti performance tiers represent guaranteed minimum performance. Performance maximums are not enforced. Higher performance workloads are prioritized over “best-effort” workloads.\nUse the following command to create a performance tier:\nFor example:\n \\$ dctl perf-tier create performance-tier1 -i 1K -b 1G\nUse the following command to delete a performance tier:\ndctl perf-tier delete \\\u0026lt;performance-tier-name\\\nNote: You cannot delete the “best-effort” performance tier.\nManaging Networks The Diamanti software enables you to manage networks as objects. A network object consists of an IP address range, the subnet, the VLAN associated with the subnet, and the default gateway.\nContainers can connect to multiple networks using endpoints. The endpoint specifies the network object and the performance tier to be used by the container.\nUse the following command to create a network:\nThe following table describes the command options:\n   Parameter Description     \u0026lt;network-name\\ The name of the network.   --subnet \u0026lt;subnet/class\\ The subnet for the network. The command defaults to the subnet you have already specified, though you can change the value, as required.   --start \u0026lt;ip-range-start\\ The starting address of the range of IP addresses to allocate to con- tainers on the network.   --end \u0026lt;ip-range-end\\ The final address of the range of IP addresses to allocate to containers on the network.   Parameter Description   --gateway \u0026lt;gateway\\ The IP address of the gateway. The command defaults to the gateway you have already specified, though you can change the value, as required.   --vlan \u0026lt;vlan-id\\ The ID of the virtual LAN on which the subnet should be placed based on your network configuration.    For example:\nYou can verify that the network was successfully created with the attributes you specified using the following command:\n\\$ dctl network list\nAfter creating a network, you can include the network by name in your pod definition files. The following highlights the relevant section of the pod definition file containing the network specification:\nThe example shows an optional performance tier assigned to the interface to enforce guaranteed performance.\nDiamanti software requires that all performance tier settings within a single pod definition file be identical. In other words, all network interfaces and storage volumes should be set to the same performance tier. If a performance tier is not specified, the Diamanti software assumes “best- effort.”\nTo delete a network from your Diamanti cluster, use the following command:\nYou can only delete networks that are not being used. You are prompted to confirm the delete operation.\nFor example, the following command removes the “northbound” network that you created earlier:\nManaging Endpoints The Diamanti software enables you to provision container networking through endpoints that associate container networking requirements with network objects and performance tiers.\nEndpoints can be either dynamic or persistent.\nDynamic endpoints are created automatically when a pod is scheduled, and are deleted when the pod terminates. These endpoints are consumed by a single pod/service, and are specified inside the pod definition file, as shown in the following:\nPersistent endpoints are named and can have either an IP address assigned automatically or have an IP address explicitly assigned. Use the following command to create a persistent endpoint, automatically assign an IP address, and assign the endpoint to a namespace:\ndctl endpoint create \\\u0026lt;endpoint-name\\ -n \\\u0026lt;network-name\\ -ns \\\u0026lt;namespace\\\nFor example:\nAlternatively, use the following command to create a persistent endpoint and explicitly assign an IP address:\ndctl endpoint create \\\u0026lt;endpoint-name\\ -n \\\u0026lt;network-name\\ -i \\\u0026lt;ip-address\\\nFor example:\nPersistent endpoints are similarly consumed by a single pod/service, and are specified inside the pod definition file, as shown in the following:\nTo list the defined endpoints, use the following command:\ndctl endpoint list\nTo show detailed information about a specific endpoint, use the following command:\ndctl endpoint get \\\u0026lt;endpoint-name\\\nUse the following command to delete a persistent endpoint:\ndctl endpoint delete \\\u0026lt;endpoint-name\\\nFor example:\n\\$ dctl endpoint delete mysqlep\nEach pod has a management network interface and, optionally, multiple data network endpoints. The management network provides connectivity from pods to nodes, the Kubernetes service, DNS, and more. Management network provisioning is automatic, but you are responsible for specifying the data endpoints.\nManaging Storage The Diamanti software enables you to create and attach multiple storage volumes to a container and optionally guarantee minimum throughput using selected performance tiers.\nUse the following command to create a storage volume:\ndctl volume create \\\u0026lt;volume-name\\ -s \\\u0026lt;size\\\nFor example:\n\\$ dctl volume create mongo-vol -s 20G\nThis creates a new volume called mongo-vol allocating 20GB of storage.\nAfter creating the storage volume, you need to modify the pod definition file to instruct the appropriate container to use the volume. For example, you could use the volume created in this section by adding the text in bold below:\nThe example shows an optional performance tier assigned to the volume to enforce guaranteed performance. You can also specify a placement constraint using the \u0026ndash;sel=\u0026lt;label\\ option to create a volume on a specific node or on a node with specific characteristics.\nDiamanti software requires that all performance tier settings within a single pod definition file be identical.\nIn other words, all network interfaces and storage volumes should be set to the same performance tier. If a performance tier is not specified, the Diamanti software assumes “best-effort.”\nThe previous example also shows how to configure the detach policy for a volume using the\ndetachPolicy option. The detach policy specifies whether a mirrored or remote volume can be\ndetached automatically when an initiator node is not reachable. Note that detaching a volume automatically can lead to data loss under certain network partitioning conditions; use this option with care.\nThe following settings are available for the detachPolicy option:\n   Setting Description     Auto (Default) The volume is force detached automatically when the initia- tor node is not reachable and the corresponding pod is evicted. This enables the workload to be moved to a different node automatically without requiring user intervention to force detach volumes. Note that pods are evicted after five minutes when a node is not reachable or fails.   Manual This is the default setting, in which volumes are not detached auto- matically when an initiator node is unreachable (even when the corre- sponding pod is evicted). Use this setting when the combination of pod eviction and a node being unreachable does not imply that the corresponding pods have to stop running immediately. In this case, since the pod might still be using the volume, an automatic force detach on the initiator could lead to data loss. Note that you can always override this setting and manually force detach volumes, as needed.    PersistentVolume and PersistentVolumeClaim The Diamanti software supports PersistentVolume (PV) and PersistentVolumeClaim (PVC). A PersistentVolume is storage that has been provisioned by an administrator and has a lifecycle independent of any individual pod that uses the PV.\nA PersistentVolumeClaim (PVC) is a request for storage by a user. PVCs consume PV resources, with a claim requesting a specific size and access mode. Volumes persist across container failures and restarts.\nEach PV contains a spec and status, which is the specification and status of the volume. For example:\nSimilarly, each PVC contains a spec and status, which is the specification and status of the claim. For example:\nThe following example shows how to specify a PVC in a pod definition file:\nThe volumeMounts name value must be the same as the volumes name setting. Also, Kubernetes requires that the volumeMounts name value must be unique within a node (and preferably across the cluster).\nDynamic Provisioning You can specify the dynamic provisioning of volumes, in which volumes are dynamically created and optionally deleted on request. To use dynamic provisioning, you begin by creating a storage class that specifies the name, file system type, performance tier, mirror count, and provisioner, as shown in the following example:\nIn this case, the provisioner is diamanti.com/default-provisioner.\nThe dynamic provisioner uses the storage classes you have defined to create requested volumes dynamically when a user creates a PVC (specifying the volume size and storage class), as shown in the following:\nMirroring The Diamanti software additionally supports mirroring (up to three-way mirrors). Each mirror or plex exists on a single node. The Diamanti scheduler auto selects and spreads the mirrors on different nodes in the cluster. You can also optionally override this auto-selection using a selector.\nUse the following command to create a mirrored volume:\ndctl volume create \u0026lt;volume-name\\ -s \u0026lt;size\\ -m \u0026lt;mirror-count\\\nFor example:\nThis creates a three-way mirrored volume.\nUse the following command to display the number of nodes for a volume:\ndctl volume describe \u0026lt;volume-name\\\nFor example:\n   vol1.p0 appserv1 Up Detached InSync     vol1.p1 appserv2 Up Detached Detached   vol1.p2 appserv3 Up Detached Detached    You can add another mirror to volumes with one or two mirrors (one mirror at a time) using the following command:\ndctl volume modify \u0026lt;volume-name\\ -m \u0026lt;new-mirror-count\\\nSimilarly, you can delete a mirror (plex) associated with a volume using the following command:\ndctl volume plex-delete \u0026lt;volume-name\\ p\u0026lt;plex-number\\\nFor example:\ndctl volume plex-delete vol1 p1\nYou can delete storage volumes that are no longer needed. Recall that persistent volumes persist across container failures and restarts.\nUse the following command to delete a storage volume:\ndctl volume delete \u0026lt;volume-name\\\nFor example:\n$ dctl volume delete mongo-vol\nNote: You cannot delete a volume that is in use. Also, deleting a volume is an irrevocable action. Exercise care when deleting a storage volume.\nDiamanti software periodically monitors all volumes and checks for active volumes in degraded state. Degraded state is an indication that one or more plexes in the volume are in Detached condition. If any are found, Diamanti software tries to auto attach the plex if the node on which the plex resides and corresponding storage links are in good condition.\nThe auto-attach frequency is configured in the /etc/diamanti/convoy.conf file using the following parameter:\nDIAMANTI_PLEX_AUTO_ATTACH_INTERVAL\nThe default value is 30 minutes. Diamanti software attempts to attach the detached plexes three times within this configured interval. If the plex attach is not successful, it is scheduled again for after the configured interval. Note that changes to this interval requires that you restart the convoy service on the master node.\nMonitoring Health The Diamanti D-Series appliance is a sophisticated platform with multiple hardware and software components. Administrators need to regularly monitor the health of D-Series nodes in the cluster. If a node is unhealthy, new pods are not scheduled to run on the node.\nThe Diamanti software automatically corrects most transient issues and restores nodes to a healthy status. However, certain hardware or software issues can cause the node to remain unhealthy for extended periods of time.\nAdministrators can monitor node status using the Nodes tab in the Diamanti UI. Administrators can also use the following commands to display the node health status:\nMonitoring Events The Diamanti software displays a list of events that you can use to monitor and manage resources in the cluster. You can also configure policies to specify the number of days to retain events and enable SNMP traps, as required.\nUse the following command to display events:\n$ dctl event list\nBy default, the 50 most recent events are displayed. You can use command options to list up to 1000 events, and optionally specify an offset allowing you to page through a larger number of events. You can also filter events by node, pod, or volume.\nUse the following command to configure event policies:\n dctl event configure --retention-days \u0026lt;n\u0026gt; --snmp-community “\u0026lt;string\u0026gt;” --snmp-enable=\u0026lt;true|false\u0026gt; --snmp-receiver=\u0026lt;ip-address-1[,ip-address-2,ip-address-3]\u0026gt; $ dctl For example:\n$ dctl event configure --retention-days 60 --snmp-enable=true --snmp-receiver=192.168.0.111\nThis command configures an event policy enabling SNMP traps with 60 day retention for events. The following shows how to configure multiple SNMP receivers:\nFinally, you can display the current event configuration using the following command:\ndctl event status\nDeploying Workloads After creating Diamanti network and storage objects, you can deploy workloads as containers using Kubernetes pod definitions. For more information about deploying workloads, refer to Diamanti D-Series Technical Note: Exploring Appliance Capabilities.\n"
},
{
	"uri": "http://www.diamanti.com/docs/cli_reference/cluster/",
	"title": "Cluster commands",
	"tags": [],
	"description": "",
	"content": "dctl cluster Manage clusters, including creating and destroying clusters. Use also to add and remove nodes in a cluster, as well as display the status of a cluster, among other operations.\nSyntax dctl [global options] cluster \u0026lt;command\u0026gt; [command options] [arguments...]  Commands  create Create a cluster. Use only once in the lifecycle of the cluster. add Add one or more nodes to the cluster. update Update one or more nodes in the cluster. remove Remove one or more nodes from the cluster. etcd-add Add one or more nodes to the etcd cluster. etcd-remove Remove one or more nodes from the etcd cluster. configure Configure the cluster. export Export the cluster configuration (JSON output format). destroy Destroy the cluster. The command prompts for confirmation. Use with care; this action cannot be undone. status Display basic cluster information and statistics of the cluster. info Display minimal information about the cluster (for unauthenticated users). Options  --help, -h Show help dctl cluster create Create a cluster.\nSyntax dctl cluster create \u0026lt;cluster-name\u0026gt; \u0026lt;node-name\u0026gt;[,\u0026lt;node-name\u0026gt;,...] [command options] [arguments...]  Options  --virtual-ip \u0026lt;cluster-virtual-ip\u0026gt;, (Required) The IP address --vip \u0026lt;cluster-virtual-ip\u0026gt; for managing the cluster. --poddns-domain \u0026lt;cluster-name\u0026gt;[.company.domain], (Required) The domain name --poddns \u0026lt;cluster-name\u0026gt;[.company.domain] for the SkyDNS service for all containers deployed in the cluster. The default is domain.com. --storage-vlan \u0026lt;vlan-id\u0026gt;, --svlan \u0026lt;vlan-id\u0026gt; (Required) The virtual LAN ID (for intra-cluster storage communication, with jumbo frames enabled). The default value is 0 (disabled). --admin-password \u0026lt;password\u0026gt;, -p \u0026lt;password\u0026gt; The password for the admin user. If a password is not specified in the command line, the command prompts for a password. The password needs to meet the Unicode standard, and must consist of at least 8 characters, including at least one special character, one upper and lowercase character, and one number. --ca-cert \u0026lt;path-to-certificate\u0026gt; The certificate authority for the cluster. --tls-cert \u0026lt;path-to-certificate\u0026gt; The TLS certificate for the cluster. --tls-key \u0026lt;path-to-key-file\u0026gt; The TLS private key for the cluster. --multizone value Enable or disable multizone support. Example dctl cluster create mycluster node-01,node-02,node-03 --vip 192.168.30.10 --poddns mycluster.diamanti.com --storage-vlan 2  dctl cluster add Add one or more nodes to the cluster.\nSyntax dctl [global options] cluster add \u0026lt;node-name\u0026gt;[,\u0026lt;node-name\u0026gt;,...] [command options] [arguments...]  Example dctl cluster add node-04  dctl cluster update certificate Update user-supplied certificates in a cluster.\nSyntax dctl [global options] cluster update certificate [command options] [arguments...]  Options  --ca-cert \u0026lt;absolute-path\u0026gt; (Required) The absolute path of the certificate. --server-cert \u0026lt;absolute-path\u0026gt; (Required) The absolute path of the server certificate. --server-key \u0026lt;absolute-path\u0026gt; (Required) The absolute path of the key for the server certificate. Example $ dctl cluster update certificate --ca-cert sample_certs1/ca.crt --server-key sample_certs1/server.key --server-cert sample_certs1/server.crt  dctl cluster update software Update the Diamanti software in the cluster.\nSyntax dctl [global options] cluster update software [command options] [arguments...]  Options  --image value, -i \u0026lt;file-path\u0026gt; (Required) The absolute path of the RPM image file on the master node. --node-list \u0026lt;node-list\u0026gt;, -l \u0026lt;node-list\u0026gt; The list of nodes to update. --force, -f Force the software update. --no-reboot, --nr Skip reboot after the software update. --abort-on-failure, -t Abort on software update failure. --no-drain, --nd Skip draining pods during software update. --parallel-count \u0026lt;count\u0026gt;, -p \u0026lt;count\u0026gt; The number of nodes to activate image in parallel. The default is 1. Example dctl cluster update software --image \u0026quot;/home/diamanti/diamanti-cx-2.0.1-1.x86_64.rpm\u0026quot;  dctl cluster update status Display the status of a Diamanti software update.\nSyntax dctl [global options] cluster update status [command options] [arguments...]  Example dctl cluster update status  dctl cluster remove Remove one or more nodes from the cluster.\nSyntax dctl [global options] cluster remove \u0026lt;node-name\u0026gt;[,\u0026lt;node-name\u0026gt;,...] [command options] [arguments...]  Options  --assume-yes, -y Assume yes for the confirmation prompt. Example dctl cluster remove node-04  dctl cluster etcd-add Add one or more nodes to the etcd cluster.\nSyntax dctl [global options] cluster etcd-add \u0026lt;node-name\u0026gt;[,\u0026lt;node-name\u0026gt;,...] [command options] [arguments...]  Example dctl cluster etcd-add node-04  dctl cluster etcd-remove Remove one or more nodes from the etcd cluster.\nSyntax dctl [global options] cluster etcd-remove \u0026lt;node-name\u0026gt;[,\u0026lt;node-name\u0026gt;,...] [command options] [arguments...]  Options  --assume-yes, -y Assume yes for the confirmation prompt. Examples dctl cluster etcd-remove node-04  dctl cluster configure Configure the cluster.\nSyntax dctl [global options] cluster configure [command options] [arguments...]  Options  --storage-vlan [vlan-id], (Required) The virtual LAN ID (for intra- --svlan [vlan-id] cluster storage communication, frames enabled). The default value is 0 (disabled). --master [new-node] Migrate the master to a different node in the cluster. --assume-yes, -y Assume yes for the confirmation prompt. Examples dctl cluster configure --svlan 22 dctl cluster configure --master appserv73 dctl cluster export Export the cluster configuration (in JSON file format).\nSyntax dctl [global options] cluster export [arguments...]  Example dctl -o json cluster export  dctl cluster destroy Destroy the cluster. The command prompts for confirmation. Use with care; this action cannot be undone.\nSyntax dctl [global options] cluster destroy [command options] [arguments...]  Options  --delete-volumes Delete the volumes on the cluster. If the volumes are not deleted, the volumes are imported when the cluster is next created. Example dctl cluster destroy 2d708d7d-dc20-11e6-84ea-a4bf01147774  dctl cluster status Display the status of the cluster.\nSyntax dctl [global options] cluster status [arguments...]  Example dctl cluster status  Output  Cluster status Name The name of the cluster. UUID The universally unique identifier of the cluster. State The state of the cluster. Version The Diamanti software version. Master The name of the cluster master. Etcd State The state of the quorum, from among the following: - Healthy — The number of healthy quorum members is greater than the quorum size and the number of members in the etcd cluster is odd. - Degraded — The number of healthy quorum members is greater than the quorum size and the number of members in the etcd cluster is even. - NoFaultTolerance — The number of healthy quorum members is equal to the quorum size. Virtual IP The cluster virtual IP address. Storage VLAN The virtual LAN ID. Pod DNS Domain The DNS server for the pod. Nodes status Name The DNS (short) name of the node. Node-Status The status of the node. K8S-Status The Kubernetes status of the node. Millicores The used and available physical CPU cores (times 1000). Memory The used and available memory. Storage The used and available storage. IOPS The used and available input/output operations per second. VNICS The number of used and available virtual network interfaces. Bandwidth The used and available bandwidth. Storage Controllers The number of used and available storage controllers. (Total, Remote) Labels The labels associated with the node. dctl cluster info Report and save information about the cluster.\nSyntax dctl [global options] cluster info [command options] [arguments...]  Options  --quiet, -q Run the command in quiet mode. --save, -s Save the cluster and certificate information, storing the information in the .dctl.d and .kube directories of the logged-in user on the local machine. In addition, the certificate authority is embedded in the ~/.dctl.d/config and ~/.kube/config files. --insecure, Skip verification of server certificate. --insecure-skip-tls-verify, -i This indicates that user-supplied certificates were used to create the cluster and the certificate authority (ca.crt) is not available for the user. This allows the user to configure dctl to ignore server certificate validation when issuing the dctl login command. --proxy, -p Configure proxy mode for Kubernetes. --assume-yes, -y Assume yes for the confirmation prompt. --ca-cert Display the CA certificate used by dctl. --k8s-ca-cert Display the CA certificate used by kubectl. --all-servers Display a list of all cluster endpoints. Note: The command only works using the VIP or FQDN. When specifying the FQDN, DNS needs to be enabled in your environment. Note also that users can save the cluster and certification information for a single Diamanti D-Series appliance only at any given time.\n"
},
{
	"uri": "http://www.diamanti.com/docs/cli_reference/config/",
	"title": "Configuration commands",
	"tags": [],
	"description": "",
	"content": "dctl config Generate cluster configuration files.\nSyntax dctl [global options] config \u0026lt;command\u0026gt; [command options] [arguments...]  Commands  set-cluster Set the cluster configuration. Options  --help, -h Show help dctl config set-cluster Manually generate a configuration file to access the Diamanti cluster. This command is equivalent to the dctl -s [node ip] cluster info --save command.\nSyntax dctl [global options] config set-cluster [command options] [arguments...]  Options  --api-version \u0026lt;version\u0026gt; (Required) The API version for dctl cluster configuration entry. The default is v1. --virtual-ip \u0026lt;cluster-virtual-ip\u0026gt;, (Required) The IP address --vip \u0026lt;cluster-virtual-ip\u0026gt; for managing the cluster. --certificate-authority \u0026lt;certificate-file\u0026gt;, (Required) The path to the --ca \u0026lt;certificate-file\u0026gt; certificate authority file whose contents are to be embedded in the configuration file. --k8s-certificate-authority \u0026lt;certificate-file\u0026gt;, The path to certificate --k8s-ca \u0026lt;certificate-file\u0026gt; authority file whose contents are to be embedded in the .kube/config file. --proxy, -p The proxy mode for the Kubernetes API server. --insecure, -i Specifies insecure mode for dctl operations (ignore the ca.crt file, if set). --dns \u0026lt;domain-name\u0026gt;, -d \u0026lt;domain-name\u0026gt; The domain name for the cluster API server. --server \u0026lt;node-name\u0026gt;, -s \u0026lt;node-name\u0026gt; The server endpoint to use for dctl operations. --server-list \u0026lt;node-name\u0026gt;[,\u0026lt;node-name\u0026gt;,...] The server endpoint List to use for dctl operations. Example dctl config set-cluster mycluster --api-version v1 --virtual-ip 192.168.30.10 --ca /etc/diamanti/certs/ca.crt  dctl drive Manage drives, including listing and formatting drives.\nSyntax dctl [global options] drive \u0026lt;command\u0026gt; [command options] [arguments...]  Commands  get Display information about a specific drive. list Display a summary of all drives in a cluster (default) or a specific node. format Format all drives on a specific node. Options  --help, -h Show help "
},
{
	"uri": "http://www.diamanti.com/docs/copyright/",
	"title": "Copyright",
	"tags": [],
	"description": "",
	"content": "Copyright © 2020 Diamanti. All rights reserved.\nIf this guide is distributed with software that includes an end user agreement, this guide, as well as the software described in it, is furnished under license and may be used or copied only in accordance with the terms of such license. Except as permitted by any such license, no part of this guide may be reproduced, stored in a retrieval system, or transmitted, in any form or by any means, electronic, mechanical, recording, or otherwise, without the prior written permission of Diamanti. Please note that the content in this guide is protected under copyright law even if it is not distributed with software that includes an end user license agreement.\n"
},
{
	"uri": "http://www.diamanti.com/_footer/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Copyright © 2020 Diamanti. All rights reserved.\n"
},
{
	"uri": "http://www.diamanti.com/_header/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://www.diamanti.com/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://www.diamanti.com/",
	"title": "Diamanti documentation",
	"tags": [],
	"description": "",
	"content": "Welcome to Diamanti! The Diamantiplatform is a set of integrated hardware and software solutions that enable transformational application performance from on-premises to hybrid cloud with Kubernetes out-of-the-box.\nDiamanti Enterprise Kubernetes Platform Diamanti’s enterprise Kubernetes platform gives platform architects, IT operations, and application owners the performance and enterprise features needed to run cloud-native applications at scale. With open-source Docker and Kubernetes fully integrated into the platform, together with purpose-built hardware and multi-cloud integration, Diamanti is full-featured Kubernetes infrastructure that deploys in minutes.\nPurpose-Built for Kubernetes Diamanti is the only provider to combine an integrated software solution that includes Docker, Kubernetes and intelligent I/O controls with a patented I/O-optimized architecture and high performance, NVMe-based hyperconverged infrastructure. Diamanti’s virtualized approach to network and storage traffic management addresses the unique requirements of cloud-native applications. At the same time, Diamanti delivers unmatched resource utilization—up to 95%—across the entire cluster. No other Kubernetes infrastructure achieves comparable performance in such a small data center footprint.\nPlug-and-play Networking Containers have their own unique system of port mappings, overlays, and bandwidth requirements that create a host of interoperability challenges. Diamanti eliminates these configuration roadblocks by integrating directly with existing network infrastructure. Each container is automatically allocated an IP address and can reside on any subnet.\nFast NVMe-based Persistent Storage Legacy scale-up storage arrays don’t fit modern scale-out containers. Trying to achieve performance at scale for databases and key value stores has ops teams scrambling to deliver persistent storage. Diamanti meets the storage needs of your stateful applications with low-latency NVMe block storage, delivering 100-microsecond read/write latency. Diamanti extends NVMe across the cluster using standard 10Gb Ethernet, offering data mobility without compromise.\nSeamless Scalability Easily scale your Kubernetes infrastructure with multi-cloud capabilities and Diamanti appliances that deliver 1,000,000+ IOPS per 1U node and sub-millisecond latency.\n24×7 Enterprise-class Support By providing single-point-of-contact support for your container stack, Diamanti allows you to focus on developing applications instead of building and maintaining infrastructure.\n"
},
{
	"uri": "http://www.diamanti.com/docs/",
	"title": "Diamanti documentation",
	"tags": [],
	"description": "",
	"content": "The Diamantiplatform is a set of integrated hardware and software solutions that enable transformational application performance from on-premises to hybrid cloud with Kubernetes out-of-the-box.\n"
},
{
	"uri": "http://www.diamanti.com/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]